{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROFAB is a benchmarking platform that provides dataset, classifies the proteins according functional annotations and evaluates the training models. The reason to do this platform is providing complete sets of dataset-training-evaluation triangle. Since the workflow is dense, an easy to implement user case is prepared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test files to apply ProFAB can be downloaded from https://drive.google.com/file/d/1slhzT7LBp_AE67XoxjIWhlbWnfRZD8_j/view?usp=sharing. After downloading them, inside folders ('ec_dataset' and 'go_dataset') should be directed into '/prb/import_dataset' directory. The reason of this extra workload is coming from size of data.\n",
    "\n",
    "To import data in Python, following lines of code can be used. It is designed as easy to implement and if user needs to import multiple dataset at the sametime a loop can be used. For example following codes can be examined:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To import single dataset from ecNo prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prb.import_dataset impor ECNO\n",
    "data_model = ECNO(ratio = 0.2, protein_feature = pf, pre_determined = True, set_type = 'target')\n",
    "X_train,X_test,X_validation,y_train,y_test,y_validation = data_model.get_data(data_name = 'ec_1-2-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To import multiple dataset from ecNo prediction data in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prb.import_dataset impor ECNO\n",
    "data_model = ECNO(ratio = 0.2, protein_feature = pf, pre_determined = True, set_type = 'random')\n",
    "ecNames = ['ec_1-2-1','ec_1-4-4-2','ec_2-7-4-6']\n",
    "for name in ecNames:\n",
    "    X_train,X_test,X_validation,y_train,y_test,y_validation = data_model.get_data(data_name = name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROFAB can train any type of data. It provides both classification and regression training. Since our datasets are based on classication of proteins, as an example, classification method is shown, however, the same process is valid for regression, too.\n",
    "\n",
    "After training session, outcome of training is stored in 'model_path'. Because this process lasts to long, saving the outcome will be time-saver. Stored model must be exported and be imported with 'pickle' Python library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To train the data:\n",
    "import pickle\n",
    "from prb.process_learn_evaluate import scale_methods, classification_methods, evaluate_score\n",
    "\n",
    "#Let's define model path where training model will be saved.\n",
    "model_path = 'model_path.txt'\n",
    "\n",
    "#Then sets are scaled to eleminate bias. Scaler is obtained from train data and can be used for different sets\n",
    "X_train,scaler = scale_methods(X_train,scale_type = 'Standard_Scaler')\n",
    "X_test,X_validation = scaler.transform(X_test),scaler.transform(X_validation)\n",
    "\n",
    "#After assigning paths and scaling datasets, training can be done manually like this way:\n",
    "classification_methods(path = model_path,ml_type = 'naive_bayes',\n",
    "                                        X_train = X_train,\n",
    "                                        y_train = y_train,\n",
    "                                        cv = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training session is done, evaluation can be done with following lines of code. The output of evaluation is given below of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get saved model, following code can be run.\n",
    "model = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "#After that, for all sets evaluation metrics can be obtained separately.\n",
    "score_train,f_train = evaluate_score(model,X_train,y_train)\n",
    "score_test,f_test = evaluate_score(model,X_test,y_test)\n",
    "score_validation,f_validation = evaluate_score(model,X_validation,y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score of train and test are given for data: 'ecNo_1-2-1', 'target'.\n",
    "```{python}\n",
    "score_train =  {Precision:0.74352651, Recall:0.914560162, F1-score:0.820222172, F05-score:0.772416738, Accuracy:0.821917808, MCC:0.661149793, TP:1809, FP:624, TN:1851, FN:169}\n",
    "\n",
    "score_train =  {Precision:0.817891, Recall:0.937729, F1-score:0.87372, F05-score:0.839344, Accuracy:0.864964, MCC:0.737965, TP:256, FP:57, TN:218, FN:17}\n",
    "\n",
    "score_train =  {Precision:0.749588138, Recall:0.913655, F1-score:0.823529, F05-score:0.777512, Accuracy:0.824955, MCC:0.665838, TP:455, FP:152, TN:464, FN:43}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Formating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the data in table format, following lines of code can be executed. Besides scores, sizes of each sets are also given. Tables is stored in .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If user wants to see all results in a table, following codes can be run:\n",
    "from prb.utils import form_table\n",
    "\n",
    "score_path = 'score_path.csv' #To save the results.\n",
    "\n",
    "scores = [score_train,score_test,score_validation]\n",
    "size_of = [str(len(X_train))  + \n",
    "            'x' + str(len(X_train[0])),str(len(X_test))  +\n",
    "            'x' + str(len(X_test[0])),str(len(X_validation))  +\n",
    "            'x' + str(len(X_validation [0]))]\n",
    "\n",
    "preds = [f_train,f_test,f_validation]\n",
    "names = ['Train','Test','Validation']\n",
    "\n",
    "\n",
    "learning_method = 'classification'\n",
    "form_table(score_path = score_path, names = names,\n",
    "         scores = scores,sizes = size_of, \n",
    "         learning_method = learning_method,preds = preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a processing with multiple datasets, all functions can be introduced in 'for' loop. In following lines can be copied and pasted to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prb.import_dataset impor ECNO\n",
    "import pickle\n",
    "from prb.process_learn_evaluate import scale_methods, classification_methods, evaluate_score\n",
    "from prb.utils import form_table\n",
    "\n",
    "data_model = ECNO(ratio = 0.2, protein_feature = pf, pre_determined = True, set_type = 'random')\n",
    "ecNames = ['ec_1-2-1','ec_1-4-4-2','ec_2-7-4-6']\n",
    "for name in ecNames:\n",
    "    X_train,X_test,X_validation,y_train,y_test,y_validation = data_model.get_data(data_name = name)\n",
    "    #To train the data:\n",
    "\n",
    "    #Let's define model path where training model will be saved.\n",
    "    model_path = 'model_path.txt'\n",
    "\n",
    "    #Then sets are scaled to eleminate bias. Scaler is obtained from train data and can be used for different sets\n",
    "    X_train,scaler = scale_methods(X_train,scale_type = 'Standard_Scaler')\n",
    "    X_test,X_validation = scaler.transform(X_test),scaler.transform(X_validation)\n",
    "\n",
    "    #After assigning paths and scaling datasets, training can be done manually like this way:\n",
    "    classification_methods(path = model_path,ml_type = 'naive_bayes',\n",
    "                                            X_train = X_train,\n",
    "                                            y_train = y_train,\n",
    "                                            cv = None)\n",
    "    #To get saved model, following code can be run.\n",
    "    model = pickle.load(open(model_path,'rb'))\n",
    "\n",
    "    #After that, for all sets evaluation metrics can be obtained separately.\n",
    "    score_train,f_train = evaluate_score(model,X_train,y_train)\n",
    "    score_test,f_test = evaluate_score(model,X_test,y_test)\n",
    "    score_validation,f_validation = evaluate_score(model,X_validation,y_validation)\n",
    "    \n",
    "    #If user wants to see all results in a table, following codes can be run:\n",
    "\n",
    "    score_path = 'score_path.csv' #To save the results.\n",
    "\n",
    "    scores = [score_train,score_test,score_validation]\n",
    "    size_of = [str(len(X_train))  + \n",
    "                'x' + str(len(X_train[0])),str(len(X_test))  +\n",
    "                'x' + str(len(X_test[0])),str(len(X_validation))  +\n",
    "                'x' + str(len(X_validation [0]))]\n",
    "\n",
    "    preds = [f_train,f_test,f_validation]\n",
    "    names = ['Train','Test','Validation']\n",
    "    \n",
    "    learning_method = 'classification'\n",
    "    form_table(score_path = score_path, names = names,\n",
    "             scores = scores,sizes = size_of, \n",
    "             learning_method = learning_method,preds = preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
